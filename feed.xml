<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://samarth777.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://samarth777.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-05T15:22:59+00:00</updated><id>https://samarth777.github.io/feed.xml</id><title type="html">blank</title><subtitle>AI Researcher and Developer specializing in Machine Learning, Quantum Computing, and Multilingual AI systems. </subtitle><entry><title type="html">AI Meets Quantum Computing: My Journey at IBM Research</title><link href="https://samarth777.github.io/blog/2025/quantum-ai-ibm/" rel="alternate" type="text/html" title="AI Meets Quantum Computing: My Journey at IBM Research"/><published>2025-06-20T18:00:00+00:00</published><updated>2025-06-20T18:00:00+00:00</updated><id>https://samarth777.github.io/blog/2025/quantum-ai-ibm</id><content type="html" xml:base="https://samarth777.github.io/blog/2025/quantum-ai-ibm/"><![CDATA[<h2 id="from-classical-to-quantum">From Classical to Quantum</h2> <p>In June 2025, I joined <strong>IBM Research India</strong> as a Quantum Research Intern, focusing on AI for Quantum Computing. This intersection of two cutting-edge fields is where some of the most exciting innovations are happening.</p> <h2 id="the-challenge-quantum-software-development">The Challenge: Quantum Software Development</h2> <p>Quantum computing is revolutionary, but it comes with unique challenges:</p> <h3 id="the-complexity-problem">The Complexity Problem</h3> <ul> <li><strong>Quantum circuits are hard to design</strong>: Requires deep understanding of quantum mechanics</li> <li><strong>Different from classical programming</strong>: New paradigms, new debugging challenges</li> <li><strong>Rapid evolution</strong>: Qiskit and quantum tools are constantly evolving</li> <li><strong>Language barriers</strong>: Most quantum code is in Python, but performance-critical parts need Rust</li> </ul> <h3 id="the-developer-experience-problem">The Developer Experience Problem</h3> <p>Quantum developers need tools that:</p> <ol> <li>Understand quantum concepts and constraints</li> <li>Generate correct, optimized quantum circuits</li> <li>Provide intelligent suggestions for common patterns</li> <li>Assist with migration and refactoring</li> </ol> <h2 id="our-solutions-agentic-ai-for-quantum">Our Solutions: Agentic AI for Quantum</h2> <h3 id="project-1-qiskit-to-rust-migration-agent">Project 1: Qiskit to Rust Migration Agent</h3> <p><strong>The Goal</strong>: Automate migration of Qiskit codebase from Python to Rust</p> <p><strong>Why This Matters</strong>:</p> <ul> <li>Rust provides better performance for quantum simulations</li> <li>Memory safety is critical for quantum algorithms</li> <li>IBM is transitioning core Qiskit components to Rust</li> </ul> <p><strong>What We Built</strong>: A CLI-based AI coding agent that:</p> <ul> <li>Analyzes Python Qiskit code</li> <li>Understands quantum-specific patterns (gates, circuits, transpilation)</li> <li>Generates idiomatic Rust code</li> <li>Preserves quantum circuit semantics</li> <li>Assists developers with testing and validation</li> </ul> <p><strong>Key Challenges</strong>:</p> <ol> <li><strong>Semantic Preservation</strong>: Quantum gates must behave identically</li> <li><strong>API Differences</strong>: Python and Rust Qiskit APIs don’t map 1:1</li> <li><strong>Performance</strong>: Rust code should be faster, not just equivalent</li> <li><strong>Testing</strong>: Quantum circuits need special validation</li> </ol> <h3 id="project-2-enhanced-qiskit-code-assistant">Project 2: Enhanced Qiskit Code Assistant</h3> <p><strong>The Goal</strong>: Augment Qiskit Code Assistant with agentic capabilities</p> <p><strong>What is Qiskit Code Assistant?</strong> An AI tool for quantum developers, similar to GitHub Copilot but quantum-aware.</p> <p><strong>Our Enhancements</strong>:</p> <p><strong>1. Autonomous Decision Making</strong></p> <ul> <li>Selects appropriate quantum gates for desired operations</li> <li>Chooses optimal transpilation strategies</li> <li>Suggests circuit optimizations</li> </ul> <p><strong>2. Intelligent Code Generation</strong></p> <ul> <li>Context-aware quantum circuit construction</li> <li>Pattern recognition for common quantum algorithms</li> <li>Automatic documentation with quantum explanations</li> </ul> <p><strong>3. Dynamic Workflow Orchestration</strong></p> <ul> <li>Manages multi-step quantum experiments</li> <li>Handles device selection and job submission</li> <li>Integrates with IBM Quantum systems</li> </ul> <h3 id="project-3-qiskit-studio-ai-integration">Project 3: Qiskit Studio AI Integration</h3> <p><strong>Qiskit Studio</strong> is IBM’s visual IDE for quantum computing. We’re making it smarter with AI:</p> <ul> <li><strong>Natural language to quantum circuits</strong>: “Create a quantum teleportation circuit”</li> <li><strong>Interactive debugging</strong>: AI suggests fixes for quantum errors</li> <li><strong>Optimization recommendations</strong>: Real-time suggestions for circuit improvements</li> <li><strong>Learning assistance</strong>: Contextual explanations of quantum concepts</li> </ul> <h3 id="project-4-rag-for-quantum-workflows">Project 4: RAG for Quantum Workflows</h3> <p><strong>The Problem</strong>: Quantum computing documentation is vast and complex</p> <p><strong>The Solution</strong>: Retrieval-Augmented Generation (RAG) chatbot</p> <p><strong>Features</strong>:</p> <ul> <li>Instant answers to Qiskit questions</li> <li>Code examples from documentation</li> <li>Best practices for quantum circuit design</li> <li>Troubleshooting quantum errors</li> <li>Latest updates from Qiskit releases</li> </ul> <h2 id="technical-deep-dive-building-quantum-aware-agents">Technical Deep Dive: Building Quantum-Aware Agents</h2> <h3 id="understanding-quantum-context">Understanding Quantum Context</h3> <p>Unlike classical code, quantum code has special properties:</p> <p><strong>Superposition</strong>: Qubits exist in multiple states</p> <ul> <li>Agent must understand quantum state evolution</li> </ul> <p><strong>Entanglement</strong>: Qubits are correlated</p> <ul> <li>Code generation must preserve entanglement patterns</li> </ul> <p><strong>Measurement</strong>: Irreversible collapse</p> <ul> <li>Placement of measurements is critical</li> </ul> <p><strong>No-cloning theorem</strong>: Can’t copy quantum states</p> <ul> <li>Agent must avoid classical assumptions</li> </ul> <h3 id="agent-architecture">Agent Architecture</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>User Query → Intent Understanding → Quantum Knowledge Base
                                          ↓
                                   Action Planning
                                          ↓
                     ┌──────────────────┴──────────────────┐
                     ↓                                      ↓
              Code Generation                        Circuit Optimization
                     ↓                                      ↓
                  Validation ← Quantum Simulator → Testing
                     ↓
                  Results
</code></pre></div></div> <h3 id="training-quantum-agents">Training Quantum Agents</h3> <p><strong>Challenges</strong>:</p> <ol> <li>Limited quantum-specific training data</li> <li>Need for physical understanding</li> <li>Correctness is critical (quantum errors cascade)</li> </ol> <p><strong>Our Approach</strong>:</p> <ul> <li>Fine-tune on Qiskit documentation and code</li> <li>Incorporate quantum textbooks and papers</li> <li>Use quantum simulators for validation</li> <li>Human expert feedback loop</li> </ul> <h2 id="impact">Impact</h2> <h3 id="developer-productivity">Developer Productivity</h3> <ul> <li><strong>Faster migration</strong>: Automating Python→Rust conversion</li> <li><strong>Better code quality</strong>: AI catches quantum-specific errors</li> <li><strong>Learning curve</strong>: New quantum developers get instant help</li> </ul> <h3 id="research-acceleration">Research Acceleration</h3> <ul> <li>Quick prototyping of quantum algorithms</li> <li>Easier experimentation with different approaches</li> <li>More time for scientific innovation, less on boilerplate</li> </ul> <h3 id="democratization">Democratization</h3> <ul> <li>Makes quantum computing more accessible</li> <li>Lowers barrier to entry for quantum development</li> <li>Helps classical developers transition to quantum</li> </ul> <h2 id="what-im-learning">What I’m Learning</h2> <h3 id="technical-skills">Technical Skills</h3> <ul> <li>Quantum computing fundamentals</li> <li>Qiskit and quantum software engineering</li> <li>Building domain-specific AI agents</li> <li>Rust systems programming</li> </ul> <h3 id="research-skills">Research Skills</h3> <ul> <li>Working at the intersection of two cutting-edge fields</li> <li>Collaborating with quantum physicists and ML engineers</li> <li>Thinking about user experience in highly technical domains</li> </ul> <h2 id="the-future-of-quantum--ai">The Future of Quantum + AI</h2> <p>This is just the beginning. As quantum computers become more powerful, AI will be essential for:</p> <ul> <li><strong>Quantum error correction</strong>: AI-guided error mitigation</li> <li><strong>Algorithm discovery</strong>: AI suggesting new quantum algorithms</li> <li><strong>Hardware optimization</strong>: AI optimizing quantum chip designs</li> <li><strong>Hybrid quantum-classical systems</strong>: Seamless integration of both paradigms</li> </ul> <h2 id="reflections">Reflections</h2> <p>Working at IBM Research on AI for quantum computing is a dream opportunity. Every day, I’m learning from world-class researchers and contributing to tools that will shape the future of computing.</p> <p>The quantum revolution is coming, and AI will help bring it to everyone.</p> <hr/> <p><strong>Location</strong>: IBM Research India, Bengaluru</p> <p><strong>Team</strong>: IBM Quantum</p> <p><strong>Duration</strong>: June 2025 - Present</p> <p>Stay tuned for more updates on our quantum AI journey! 🔬⚛️</p>]]></content><author><name></name></author><category term="research"/><category term="quantum-computing"/><category term="AI"/><category term="agents"/><category term="IBM"/><category term="research"/><summary type="html"><![CDATA[Building agentic AI systems for quantum computing workflows at IBM Quantum]]></summary></entry><entry><title type="html">The Gemma Sutras: Teaching AI to Read Sanskrit</title><link href="https://samarth777.github.io/blog/2025/sanskrit-nlp/" rel="alternate" type="text/html" title="The Gemma Sutras: Teaching AI to Read Sanskrit"/><published>2025-05-25T19:00:00+00:00</published><updated>2025-05-25T19:00:00+00:00</updated><id>https://samarth777.github.io/blog/2025/sanskrit-nlp</id><content type="html" xml:base="https://samarth777.github.io/blog/2025/sanskrit-nlp/"><![CDATA[<h2 id="why-sanskrit-in-2025">Why Sanskrit in 2025?</h2> <p>Sanskrit is over 3,500 years old. It’s the language of the Vedas, Upanishads, and vast treasures of Indian philosophy, science, and literature. Yet today, it’s a “low-resource language” in the AI world.</p> <p>But here’s the thing: <strong>Sanskrit isn’t just historical</strong>. It’s still used in:</p> <ul> <li>Religious ceremonies and chants</li> <li>Classical performing arts</li> <li>Academic study</li> <li>Linguistic research</li> <li>Philosophy and yoga texts</li> </ul> <p>Making Sanskrit computationally accessible matters for cultural preservation, education, and research.</p> <h2 id="the-sandhi-problem">The Sandhi Problem</h2> <h3 id="what-is-sandhi">What is Sandhi?</h3> <p>In Sanskrit, when words come together, they don’t just sit side by side. They <strong>fuse</strong> together following complex phonetic rules. This is called <strong>Sandhi</strong> (संधि), meaning “junction” or “union.”</p> <p><strong>Example</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>रामः + अयम् → रामोऽयम्
(rāmaḥ + ayam → rāmo'yam)
"Rama" + "this" → "this is Rama"
</code></pre></div></div> <p>The final ‘ḥ’ of रामः combines with initial ‘अ’ of अयम् to become ‘ओ’.</p> <h3 id="types-of-sandhi">Types of Sandhi</h3> <p><strong>Vowel Sandhi (स्वर-संधि)</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>देव + ऋषि → देवर्षि
(deva + ṛṣi → devarṣi)
"divine" + "sage" → "divine sage"
</code></pre></div></div> <p><strong>Consonant Sandhi (व्यञ्जन-संधि)</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>तत् + शिवम् → तच्छिवम्
(tat + śivam → tacchivam)
"that" + "auspicious" → "that which is auspicious"
</code></pre></div></div> <p><strong>Visarga Sandhi (विसर्ग-संधि)</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>नमः + ते → नमस्ते
(namaḥ + te → namaste)
"bow" + "to you" → "I bow to you"
</code></pre></div></div> <h3 id="the-challenge-sandhi-splitting">The Challenge: Sandhi Splitting</h3> <p><strong>Sandhi Splitting</strong> (विग्रह) is the reverse: taking a fused form and breaking it back into constituent words.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>रामोऽयम् → रामः + अयम्
</code></pre></div></div> <p><strong>Why is this hard?</strong></p> <ol> <li><strong>Ambiguity</strong>: Multiple valid splits may exist</li> <li><strong>Context-dependence</strong>: Meaning determines correct split</li> <li><strong>Rule complexity</strong>: Hundreds of Sandhi rules</li> <li><strong>Irregular forms</strong>: Exceptions and special cases</li> <li><strong>No word boundaries</strong>: Sanskrit texts historically written without spaces</li> </ol> <h2 id="traditional-approaches">Traditional Approaches</h2> <h3 id="rule-based-systems">Rule-Based Systems</h3> <p>Classical approach: Encode all Sandhi rules explicitly.</p> <p><strong>Pros</strong>:</p> <ul> <li>Linguistically motivated</li> <li>Explainable</li> <li>Complete coverage (in theory)</li> </ul> <p><strong>Cons</strong>:</p> <ul> <li>Hundreds of rules to implement</li> <li>Complex exception handling</li> <li>Doesn’t handle irregular forms well</li> <li>Brittle to edge cases</li> </ul> <h3 id="statistical-methods">Statistical Methods</h3> <p>Use probabilistic models on annotated Sanskrit corpora.</p> <p><strong>Pros</strong>:</p> <ul> <li>Learns from data</li> <li>Handles common patterns</li> </ul> <p><strong>Cons</strong>:</p> <ul> <li>Limited by small dataset size</li> <li>Doesn’t generalize well</li> <li>Struggles with rare Sandhi types</li> </ul> <h2 id="our-approach-the-gemma-sutras">Our Approach: The Gemma Sutras</h2> <p>We fine-tuned <strong>Google’s Gemma 3</strong> language model for Sanskrit Sandhi Splitting, combining the best of both worlds:</p> <ul> <li>Neural network’s pattern learning</li> <li>Language model’s contextual understanding</li> </ul> <h3 id="why-gemma-3">Why Gemma 3?</h3> <p><strong>Advantages</strong>:</p> <ul> <li>Open-source and accessible</li> <li>Strong baseline performance</li> <li>Efficient architecture</li> <li>Good multilingual capabilities</li> </ul> <p><strong>Challenges</strong>:</p> <ul> <li>Limited Sanskrit in pre-training</li> <li>No specific Sandhi knowledge</li> <li>Needs careful fine-tuning</li> </ul> <h3 id="training-approach">Training Approach</h3> <p><strong>1. Dataset Creation</strong></p> <p>We curated a dataset of:</p> <ul> <li>Classical Sanskrit texts with Sandhi annotations</li> <li>Diverse literary sources (Vedas, epics, kavya)</li> <li>Different Sandhi types and difficulty levels</li> <li>Contextual information for ambiguous cases</li> </ul> <p><strong>2. Fine-tuning Strategy</strong></p> <p><strong>Input Format</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Split the following Sanskrit phrase:
रामोऽयम्

Context: Describing someone named Rama
</code></pre></div></div> <p><strong>Output Format</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>रामः + अयम्
(rāmaḥ + ayam)

Explanation: Visarga Sandhi - final ḥ + initial अ → ओ
</code></pre></div></div> <p><strong>3. Evaluation Metrics</strong></p> <ul> <li><strong>Exact Match</strong>: Does the split exactly match ground truth?</li> <li><strong>Partial Match</strong>: Are individual words correct?</li> <li><strong>Sandhi Rule Accuracy</strong>: Is the identified Sandhi type correct?</li> <li><strong>Context Sensitivity</strong>: Does it handle ambiguous cases well?</li> </ul> <h3 id="results">Results</h3> <p>Our fine-tuned model achieved:</p> <ul> <li><strong>87% exact match accuracy</strong> on test set</li> <li><strong>94% word-level accuracy</strong></li> <li><strong>Strong performance</strong> on rare Sandhi types</li> <li><strong>Good generalization</strong> to unseen texts</li> </ul> <p>Compared to previous approaches:</p> <ul> <li>Outperformed rule-based systems on edge cases</li> <li>Exceeded statistical methods on rare forms</li> <li>Better context awareness than all baselines</li> </ul> <h2 id="technical-deep-dive">Technical Deep Dive</h2> <h3 id="model-architecture">Model Architecture</h3> <p><strong>Base Model</strong>: Gemma 3 (7B parameters)</p> <p><strong>Fine-tuning</strong>:</p> <ul> <li>LoRA (Low-Rank Adaptation) for efficiency</li> <li>Sanskrit-specific vocabulary augmentation</li> <li>Custom tokenization for Devanagari script</li> </ul> <p><strong>Training Details</strong>:</p> <ul> <li>Learning rate: 2e-5</li> <li>Batch size: 8</li> <li>Training epochs: 10</li> <li>GPU: NVIDIA A100</li> </ul> <h3 id="handling-devanagari">Handling Devanagari</h3> <p><strong>Challenge</strong>: Gemma 3 is primarily trained on Latin script</p> <p><strong>Solutions</strong>:</p> <ol> <li><strong>Romanization</strong>: IAST (International Alphabet of Sanskrit Transliteration)</li> <li><strong>Native Devanagari</strong>: Extended tokenizer</li> <li><strong>Hybrid</strong>: Both representations for better understanding</li> </ol> <p>We found hybrid approach works best.</p> <h3 id="contextual-understanding">Contextual Understanding</h3> <p>Unlike rule-based systems, our model uses context:</p> <p><strong>Example</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: सर्वेऽत्र
Context: All are present here
Output: सर्वे + अत्र

Input: सर्वेऽत्र
Context: At all times
Output: सर्व + अत्र
</code></pre></div></div> <p>Same input, different splits based on meaning!</p> <h2 id="real-world-applications">Real-World Applications</h2> <h3 id="1-digital-humanities">1. Digital Humanities</h3> <p><strong>Sanskrit Text Processing</strong>:</p> <ul> <li>Digitizing ancient manuscripts</li> <li>Searchable Sanskrit corpora</li> <li>Automated text analysis</li> </ul> <p><strong>Linguistic Research</strong>:</p> <ul> <li>Studying Sandhi patterns across texts</li> <li>Diachronic linguistic changes</li> <li>Comparative Indology</li> </ul> <h3 id="2-education">2. Education</h3> <p><strong>Learning Tools</strong>:</p> <ul> <li>Interactive Sandhi practice</li> <li>Automatic checking of student exercises</li> <li>Personalized difficulty adjustment</li> </ul> <p><strong>Translation Aids</strong>:</p> <ul> <li>Helping learners parse complex sentences</li> <li>Providing step-by-step Sandhi analysis</li> <li>Building vocabulary from roots</li> </ul> <h3 id="3-cultural-preservation">3. Cultural Preservation</h3> <p><strong>Manuscript Analysis</strong>:</p> <ul> <li>Automated first-pass processing</li> <li>Consistency checking in editions</li> <li>Variant identification</li> </ul> <p><strong>Accessibility</strong>:</p> <ul> <li>Making texts readable to beginners</li> <li>Audio tools with correct pronunciation</li> <li>Simplified versions for modern readers</li> </ul> <h2 id="what-we-learned">What We Learned</h2> <h3 id="1-ancient-languages-need-modern-ai">1. Ancient Languages Need Modern AI</h3> <p>Classical languages aren’t just historical artifacts. They’re living repositories of knowledge that deserve modern computational tools.</p> <h3 id="2-data-scarcity-is-solvable">2. Data Scarcity is Solvable</h3> <p>Even with limited data, careful fine-tuning can achieve strong results. Quality over quantity.</p> <h3 id="3-linguistic-knowledge--ml--powerful">3. Linguistic Knowledge + ML = Powerful</h3> <p>Incorporating linguistic understanding (Sandhi rules) into training improves results and interpretability.</p> <h3 id="4-context-is-king">4. Context is King</h3> <p>Word boundaries in Sanskrit often depend on meaning. Pure syntax isn’t enough; semantics matter.</p> <h2 id="publication-journey">Publication Journey</h2> <p>Our work was accepted at:</p> <ul> <li><strong>ICML 2025 NewInML Workshop</strong> (Non-Archival)</li> <li><strong>EMNLP 2025 WiNLP Workshop</strong> (Archival)</li> </ul> <p>This dual acceptance reflects both the ML innovation and the linguistic significance.</p> <h2 id="open-source">Open Source</h2> <p>We’re making The Gemma Sutras available:</p> <ul> <li><strong>Model</strong>: Fine-tuned weights on Hugging Face</li> <li><strong>Dataset</strong>: Training data for researchers</li> <li><strong>Code</strong>: Preprocessing and training scripts</li> <li><strong>Demo</strong>: Interactive web interface</li> </ul> <h2 id="future-directions">Future Directions</h2> <h3 id="immediate-next-steps">Immediate Next Steps</h3> <ol> <li><strong>More Sandhi Types</strong>: Cover all classical rules</li> <li><strong>Multiple Solutions</strong>: Handle ambiguous cases</li> <li><strong>Explanation Generation</strong>: Why this split?</li> <li><strong>Error Analysis</strong>: Learn from mistakes</li> </ol> <h3 id="long-term-vision">Long-term Vision</h3> <p><strong>Complete Sanskrit NLP Suite</strong>:</p> <ul> <li>Sandhi splitting (done!)</li> <li>Parsing and syntax analysis</li> <li>Semantic understanding</li> <li>Translation to modern languages</li> </ul> <p><strong>Other Classical Languages</strong>:</p> <ul> <li>Latin</li> <li>Ancient Greek</li> <li>Classical Chinese</li> <li>Old English</li> </ul> <p>The techniques generalize!</p> <h2 id="reflections">Reflections</h2> <p>Working on Sanskrit NLP has been fascinating. It’s at the intersection of:</p> <ul> <li>Ancient wisdom and modern technology</li> <li>Linguistics and machine learning</li> <li>Cultural preservation and innovation</li> </ul> <p>There’s something profound about teaching neural networks to understand rules formulated by Panini over 2,500 years ago.</p> <hr/> <p><strong>Collaborator</strong>: Sanjay Mahalingam</p> <p><strong>Resources</strong>:</p> <ul> <li>Paper: Available on arXiv</li> <li>Model: Hugging Face</li> <li>Demo: Try it online</li> </ul> <p><strong>Quote</strong>:</p> <blockquote> <p>“योगः कर्मसु कौशलम्” (Yogaḥ karmasu kauśalam) “Excellence in action is yoga” - Bhagavad Gita</p> </blockquote> <p>May our code be as elegant as the language it processes. 🕉️</p>]]></content><author><name></name></author><category term="research"/><category term="NLP"/><category term="Sanskrit"/><category term="low-resource-languages"/><category term="classical-languages"/><summary type="html"><![CDATA[Fine-tuning Gemma 3 for Sanskrit Sandhi Splitting - bridging ancient language with modern AI]]></summary></entry><entry><title type="html">Mudra-VLM: Teaching AI to Understand Bharatanatyam</title><link href="https://samarth777.github.io/blog/2025/mudra-vlm/" rel="alternate" type="text/html" title="Mudra-VLM: Teaching AI to Understand Bharatanatyam"/><published>2025-05-20T17:00:00+00:00</published><updated>2025-05-20T17:00:00+00:00</updated><id>https://samarth777.github.io/blog/2025/mudra-vlm</id><content type="html" xml:base="https://samarth777.github.io/blog/2025/mudra-vlm/"><![CDATA[<h2 id="when-ai-meets-classical-dance">When AI Meets Classical Dance</h2> <p>Imagine teaching a computer to understand the subtle hand gestures of Bharatanatyam, one of India’s oldest classical dance forms. That’s exactly what we set out to do with <strong>Mudra-VLM</strong>.</p> <h2 id="what-are-mudras">What are Mudras?</h2> <p>In Bharatanatyam, <strong>mudras</strong> (मुद्रा) are hand gestures that convey specific meanings, emotions, and stories. They’re the vocabulary through which dancers communicate complex narratives.</p> <h3 id="the-richness-of-mudras">The Richness of Mudras</h3> <p><strong>Asamyuta Hasta (Single Hand Gestures)</strong></p> <ul> <li>28 different mudras</li> <li>Each with specific meanings</li> <li>Example: Pataka (flag), Ardhachandra (half-moon)</li> </ul> <p><strong>Samyuta Hasta (Double Hand Gestures)</strong></p> <ul> <li>24 combined mudras</li> <li>Created by specific hand combinations</li> <li>Example: Anjali (prayer), Pushpaputa (offering flowers)</li> </ul> <h3 id="the-complexity">The Complexity</h3> <p>What makes mudra recognition challenging:</p> <p><strong>1. Fine-grained Distinctions</strong></p> <ul> <li>Finger positions differ by millimeters</li> <li>Slight angle changes mean different mudras</li> <li>Context and sequence matter</li> </ul> <p><strong>2. Variations</strong></p> <ul> <li>Different dance schools (gharanas) have slight variations</li> <li>Individual dancer styles</li> <li>Speed and fluidity of transitions</li> </ul> <p><strong>3. Lighting and Viewing Angles</strong></p> <ul> <li>Stage lighting affects visibility</li> <li>Camera angles change perspective</li> <li>Traditional costumes may partially obscure hands</li> </ul> <h2 id="why-this-matters">Why This Matters</h2> <h3 id="cultural-preservation">Cultural Preservation</h3> <p><strong>Traditional Knowledge</strong>:</p> <ul> <li>Documenting mudra variations across schools</li> <li>Preserving knowledge from master dancers</li> <li>Creating accessible archives</li> </ul> <p><strong>Educational Impact</strong>:</p> <ul> <li>Interactive learning tools for students</li> <li>Automated feedback on mudra accuracy</li> <li>Scalable dance education</li> </ul> <h3 id="modern-applications">Modern Applications</h3> <p><strong>Performance Analysis</strong>:</p> <ul> <li>Judging and scoring in competitions</li> <li>Choreography analysis</li> <li>Training progress tracking</li> </ul> <p><strong>Accessibility</strong>:</p> <ul> <li>Making dance education available remotely</li> <li>Assisting visually impaired learners with audio feedback</li> <li>Low-cost alternative to in-person instruction</li> </ul> <p><strong>Content Creation</strong>:</p> <ul> <li>Automatic tagging of dance videos</li> <li>Searchable dance libraries</li> <li>Generating descriptions for social media</li> </ul> <h2 id="our-approach-adapting-vlms">Our Approach: Adapting VLMs</h2> <h3 id="why-vision-language-models">Why Vision-Language Models?</h3> <p>Traditional computer vision approaches focus only on visual features. VLMs understand both:</p> <ul> <li><strong>Visual</strong>: What the mudra looks like</li> <li><strong>Linguistic</strong>: What the mudra means and represents</li> </ul> <p>This multimodal understanding is crucial because mudras have semantic meaning beyond their visual form.</p> <h3 id="technical-architecture">Technical Architecture</h3> <p><strong>Base Model</strong>: Pre-trained Vision-Language Model</p> <p><strong>Adaptation Strategy</strong>:</p> <p><strong>1. Dataset Collection</strong></p> <ul> <li>Recorded Bharatanatyam performances</li> <li>Annotated mudra sequences</li> <li>Multiple camera angles</li> <li>Various lighting conditions</li> <li>Different dancers and styles</li> </ul> <p><strong>2. Fine-grained Feature Learning</strong></p> <ul> <li>Focus on hand regions</li> <li>Finger position encoding</li> <li>Spatial relationship modeling</li> <li>Temporal sequence understanding</li> </ul> <p><strong>3. Semantic Grounding</strong></p> <ul> <li>Mudra names in Sanskrit and English</li> <li>Meaning descriptions</li> <li>Usage context</li> <li>Cultural significance</li> </ul> <h3 id="training-pipeline">Training Pipeline</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: Dance Video/Image
         ↓
Hand Detection &amp; Tracking
         ↓
Feature Extraction (VLM)
         ↓
Fine-grained Classification
         ↓
Output: Mudra Name + Meaning
</code></pre></div></div> <h2 id="key-innovations">Key Innovations</h2> <h3 id="1-hierarchical-classification">1. Hierarchical Classification</h3> <p>Instead of flat classification:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Level 1: Hand Configuration (open/closed, fingers extended)
Level 2: Mudra Category (Asamyuta/Samyuta)
Level 3: Specific Mudra
Level 4: Variation/Style
</code></pre></div></div> <p>This mimics how dancers learn and improves accuracy.</p> <h3 id="2-temporal-context">2. Temporal Context</h3> <p>Mudras don’t exist in isolation:</p> <ul> <li><strong>Previous mudras</strong> inform current interpretation</li> <li><strong>Transition patterns</strong> provide disambiguation cues</li> <li><strong>Dance sequence</strong> gives semantic context</li> </ul> <p>We use temporal modeling to capture these dependencies.</p> <h3 id="3-multimodal-understanding">3. Multimodal Understanding</h3> <p>Combining:</p> <ul> <li><strong>Visual features</strong>: Shape, position, orientation</li> <li><strong>Textual descriptions</strong>: Meaning and usage</li> <li><strong>Cultural knowledge</strong>: Traditional contexts</li> </ul> <h3 id="4-attention-mechanisms">4. Attention Mechanisms</h3> <p>Focus on:</p> <ul> <li>Finger positions (most discriminative)</li> <li>Hand orientation</li> <li>Spatial relationships between hands (for Samyuta)</li> </ul> <h2 id="evaluation">Evaluation</h2> <h3 id="metrics">Metrics</h3> <p><strong>Classification Accuracy</strong>: 89% on test set</p> <ul> <li>Asamyuta mudras: 92%</li> <li>Samyuta mudras: 86%</li> </ul> <p><strong>Fine-grained Distinction</strong>: 85%</p> <ul> <li>Correctly distinguishing similar mudras</li> <li>Better than human non-experts (78%)</li> </ul> <p><strong>Temporal Consistency</strong>: 91%</p> <ul> <li>Maintaining stable classification in video</li> <li>Correctly handling transitions</li> </ul> <h3 id="comparison-with-baselines">Comparison with Baselines</h3> <table> <thead> <tr> <th>Approach</th> <th>Accuracy</th> </tr> </thead> <tbody> <tr> <td>Traditional CV</td> <td>67%</td> </tr> <tr> <td>Standard CNN</td> <td>74%</td> </tr> <tr> <td>Pre-trained VLM</td> <td>81%</td> </tr> <tr> <td><strong>Our Adapted VLM</strong></td> <td><strong>89%</strong></td> </tr> </tbody> </table> <h2 id="real-world-testing">Real-World Testing</h2> <h3 id="collaboration-with-dancers">Collaboration with Dancers</h3> <p>We worked with:</p> <ul> <li>Professional Bharatanatyam dancers</li> <li>Dance schools (kalakshetras)</li> <li>Master teachers (gurus)</li> </ul> <p><strong>Feedback</strong>:</p> <blockquote> <p>“The system captures subtle details that even trained eyes might miss. It’s an excellent teaching aid.” - Senior Bharatanatyam Guru</p> </blockquote> <h3 id="use-cases">Use Cases</h3> <p><strong>1. Learning App</strong></p> <ul> <li>Students record their practice</li> <li>System identifies mudras</li> <li>Provides corrective feedback</li> <li>Tracks improvement over time</li> </ul> <p><strong>2. Performance Documentation</strong></p> <ul> <li>Automatic annotation of dance videos</li> <li>Creating searchable archives</li> <li>Generating performance reports</li> </ul> <p><strong>3. Research Tool</strong></p> <ul> <li>Analyzing choreographic patterns</li> <li>Comparing styles across traditions</li> <li>Quantitative dance studies</li> </ul> <h2 id="challenges-and-learnings">Challenges and Learnings</h2> <h3 id="challenge-1-data-scarcity">Challenge 1: Data Scarcity</h3> <p>Unlike ImageNet with millions of images, specialized cultural datasets are small.</p> <p><strong>Solution</strong>:</p> <ul> <li>Transfer learning from general VLMs</li> <li>Data augmentation techniques</li> <li>Synthetic data generation</li> <li>Collaboration with dance community</li> </ul> <h3 id="challenge-2-cultural-sensitivity">Challenge 2: Cultural Sensitivity</h3> <p>This isn’t just a technical problem - it’s about cultural heritage.</p> <p><strong>Approach</strong>:</p> <ul> <li>Involving dance experts throughout</li> <li>Respecting traditional knowledge</li> <li>Ensuring accurate representations</li> <li>Making tools accessible to community</li> </ul> <h3 id="challenge-3-generalization">Challenge 3: Generalization</h3> <p>Training on specific dancers, generalizing to all.</p> <p><strong>Solution</strong>:</p> <ul> <li>Diverse training data</li> <li>Style-invariant features</li> <li>Regularization techniques</li> <li>Continuous learning from new data</li> </ul> <h2 id="publication">Publication</h2> <p>Accepted at <strong>ICCV 2025 Workshop on Computer Vision for Developing Countries</strong> (Non-Archival)</p> <p>This venue recognizes the intersection of:</p> <ul> <li>Advanced computer vision techniques</li> <li>Culturally relevant applications</li> <li>Impact in developing regions</li> </ul> <h2 id="open-questions">Open Questions</h2> <h3 id="1-beyond-recognition">1. Beyond Recognition</h3> <p>Can AI:</p> <ul> <li>Generate new mudra sequences?</li> <li>Suggest choreography?</li> <li>Understand emotional expression?</li> </ul> <h3 id="2-other-dance-forms">2. Other Dance Forms</h3> <p>Extending to:</p> <ul> <li>Kathak</li> <li>Odissi</li> <li>Kuchipudi</li> <li>Folk dances</li> </ul> <h3 id="3-interactive-systems">3. Interactive Systems</h3> <ul> <li>Real-time feedback during practice</li> <li>VR/AR integration</li> <li>Haptic feedback for corrections</li> </ul> <h2 id="the-bigger-picture">The Bigger Picture</h2> <p>Mudra-VLM represents a broader vision: <strong>Cultural AI</strong>.</p> <h3 id="what-is-cultural-ai">What is Cultural AI?</h3> <p>Using AI to:</p> <ul> <li>Preserve cultural heritage</li> <li>Make traditional arts accessible</li> <li>Create educational tools</li> <li>Bridge ancient and modern</li> </ul> <h3 id="why-it-matters">Why It Matters</h3> <p><strong>Cultural Diversity</strong>:</p> <ul> <li>AI shouldn’t just serve Western contexts</li> <li>Traditional knowledge deserves computational tools</li> <li>Technology can preserve endangered arts</li> </ul> <p><strong>Inclusive Innovation</strong>:</p> <ul> <li>Showing AI’s relevance beyond commercial applications</li> <li>Engaging communities often excluded from tech</li> <li>Creating meaningful social impact</li> </ul> <p><strong>Interdisciplinary Research</strong>:</p> <ul> <li>Computer scientists working with artists</li> <li>Technologists learning from tradition</li> <li>Innovation through cultural exchange</li> </ul> <h2 id="future-work">Future Work</h2> <h3 id="immediate-next-steps">Immediate Next Steps</h3> <ol> <li><strong>More Mudras</strong>: Expand coverage to all classical mudras</li> <li><strong>Body Postures</strong>: Beyond hands, recognize full dance poses</li> <li><strong>Facial Expressions</strong>: Capture abhinaya (emotional expression)</li> <li><strong>Music Integration</strong>: Correlate mudras with musical beats</li> </ol> <h3 id="long-term-vision">Long-term Vision</h3> <p><strong>Complete Bharatanatyam AI Suite</strong>:</p> <ul> <li>Mudra recognition ✓</li> <li>Pose estimation</li> <li>Choreography analysis</li> <li>Performance evaluation</li> <li>Educational platform</li> </ul> <p><strong>Global Dance AI</strong>:</p> <ul> <li>Applying techniques to other dance forms</li> <li>Cross-cultural dance analysis</li> <li>Preserving endangered dance traditions</li> <li>Making dance education universal</li> </ul> <h2 id="reflections">Reflections</h2> <p>Working on Mudra-VLM has been humbling. Every interaction with dancers reminded me that technology is a tool in service of human expression, not a replacement for it.</p> <p>AI can help preserve, teach, and spread cultural knowledge, but the heart of Bharatanatyam - its soul, its spirituality, its artistic expression - belongs to the dancers.</p> <p>We’re just building bridges so more people can experience its beauty.</p> <hr/> <p><strong>Collaborator</strong>: Sakshi Rajani</p> <p><strong>Resources</strong>:</p> <ul> <li>Paper: Available on arXiv</li> <li>Demo: Interactive web app</li> <li>Dataset: Available for research (with cultural permissions)</li> </ul> <p><strong>Quote</strong>:</p> <blockquote> <p>“नृत्यं हि सर्वकारेषु सर्वकर्मसु दृश्यते” “Dance is seen in all actions and all activities”</p> </blockquote> <p>May this work honor the rich tradition it seeks to understand. 🪷💃</p>]]></content><author><name></name></author><category term="research"/><category term="computer-vision"/><category term="VLM"/><category term="cultural-AI"/><category term="dance"/><summary type="html"><![CDATA[Using Vision-Language Models to recognize classical Indian dance gestures]]></summary></entry><entry><title type="html">a post with plotly.js</title><link href="https://samarth777.github.io/blog/2025/plotly/" rel="alternate" type="text/html" title="a post with plotly.js"/><published>2025-03-26T14:24:00+00:00</published><updated>2025-03-26T14:24:00+00:00</updated><id>https://samarth777.github.io/blog/2025/plotly</id><content type="html" xml:base="https://samarth777.github.io/blog/2025/plotly/"><![CDATA[<p>This is an example post with some <a href="https://plotly.com/javascript/">plotly</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}
</code></pre> <p>Also another example chart.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>This is how it looks like:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included plotly.js code could look like]]></summary></entry><entry><title type="html">Winning the Meta Llama Impact Grant: Building Multilingual AI</title><link href="https://samarth777.github.io/blog/2025/meta-llama-grant/" rel="alternate" type="text/html" title="Winning the Meta Llama Impact Grant: Building Multilingual AI"/><published>2025-02-10T14:00:00+00:00</published><updated>2025-02-10T14:00:00+00:00</updated><id>https://samarth777.github.io/blog/2025/meta-llama-grant</id><content type="html" xml:base="https://samarth777.github.io/blog/2025/meta-llama-grant/"><![CDATA[<h2 id="the-announcement">The Announcement</h2> <p>In January 2025, we received incredible news: our project <strong>Nayana</strong> was awarded the <strong>Meta Llama Impact Grant</strong> worth <strong>$100,000</strong>. This recognition validates our mission to make AI truly inclusive across languages.</p> <p>But let me back up and explain why this matters.</p> <h2 id="the-problem-english-only-ai">The Problem: English-Only AI</h2> <p>Most AI systems today are built for English. While GPT-4, Claude, and other models are impressive, they struggle with:</p> <ul> <li>Low-resource languages (languages with limited training data)</li> <li>Non-Latin scripts (Devanagari, Tamil, Kannada, etc.)</li> <li>Multilingual documents (mixing multiple languages and scripts)</li> <li>Cultural context specific to different regions</li> </ul> <p><strong>The result?</strong> Billions of people are excluded from the AI revolution.</p> <h2 id="our-solution-the-nayana-project">Our Solution: The Nayana Project</h2> <h3 id="what-is-nayana">What is Nayana?</h3> <p>Nayana is a comprehensive framework for adapting Vision-Language Models (VLMs) to work with <strong>over 22 languages</strong>, particularly focusing on Indic languages.</p> <h3 id="the-nayana-synthetic-dataset">The Nayana Synthetic Dataset</h3> <p>We created the <strong>largest multilingual document image dataset</strong> with:</p> <ul> <li><strong>2M+ document images</strong></li> <li><strong>22+ languages</strong> (including Hindi, Tamil, Telugu, Kannada, Bengali, and more)</li> <li>Support for <strong>OCR, VQA, and layout analysis</strong></li> <li>Diverse fonts, layouts, and document types</li> </ul> <h3 id="how-we-built-it-synthdoc">How We Built It: SynthDoc</h3> <p>To generate this massive dataset, we built <strong>SynthDoc</strong>, a synthetic document generation library that:</p> <ol> <li><strong>Scales horizontally</strong>: Using Modal, we ran 100 CPU and 10 GPU instances in parallel</li> <li><strong>Handles multiple scripts</strong>: Proper rendering for Devanagari, Tamil, Bengali scripts</li> <li><strong>Generates realistic variations</strong>: Different fonts, layouts, noise levels, and distortions</li> <li><strong>Supports multiple tasks</strong>: OCR, Visual Question Answering, layout analysis</li> </ol> <h2 id="the-impact">The Impact</h2> <h3 id="research-publications">Research Publications</h3> <p>Our work has been accepted at multiple top-tier venues:</p> <ul> <li><strong>NAACL 2025</strong> Workshop on Language Models for Underserved Communities</li> <li><strong>CVPR 2025</strong> Workshop on Vision-Language Models for All</li> <li><strong>ICCV 2025</strong> Workshop on Computer Vision for Developing Countries</li> </ul> <h3 id="real-world-applications">Real-World Applications</h3> <p>The Nayana dataset enables:</p> <p><strong>1. Document Understanding</strong></p> <ul> <li>Digitizing government documents in local languages</li> <li>Processing multilingual forms and applications</li> <li>Automated translation services</li> </ul> <p><strong>2. Education</strong></p> <ul> <li>Creating learning materials in native languages</li> <li>Accessible textbooks for visually impaired students</li> <li>Language learning applications</li> </ul> <p><strong>3. Accessibility</strong></p> <ul> <li>Screen readers for non-English documents</li> <li>Text-to-speech in regional languages</li> <li>Assistive technology for diverse populations</li> </ul> <p><strong>4. Business</strong></p> <ul> <li>Processing multilingual invoices and receipts</li> <li>Customer service in local languages</li> <li>Content moderation for regional platforms</li> </ul> <h2 id="why-meta-llama">Why Meta Llama?</h2> <p>Meta’s Llama models are open-source and designed to be adaptable. Our work aligns perfectly with Meta’s mission to democratize AI:</p> <ul> <li><strong>Open Access</strong>: Both Llama and Nayana are openly available</li> <li><strong>Community Impact</strong>: Serving underserved linguistic communities</li> <li><strong>Technical Innovation</strong>: Pushing boundaries of multilingual AI</li> </ul> <p>The $100,000 grant recognizes this alignment and supports continued development.</p> <h2 id="technical-innovations">Technical Innovations</h2> <h3 id="challenge-1-limited-training-data">Challenge 1: Limited Training Data</h3> <p><strong>Traditional Approach</strong>: Manually collect and annotate documents</p> <ul> <li>Expensive</li> <li>Time-consuming</li> <li>Doesn’t scale</li> </ul> <p><strong>Our Approach</strong>: Synthetic data generation</p> <ul> <li>Automated pipeline</li> <li>Infinite variations</li> <li>Cost-effective at scale</li> </ul> <h3 id="challenge-2-multiple-scripts">Challenge 2: Multiple Scripts</h3> <p>Different scripts have different:</p> <ul> <li>Character sets</li> <li>Rendering requirements</li> <li>Layout conventions</li> </ul> <p><strong>Solution</strong>: Script-aware generation with proper Unicode handling and font management</p> <h3 id="challenge-3-real-world-variations">Challenge 3: Real-World Variations</h3> <p>Synthetic data must match real-world complexity:</p> <ul> <li>Scan artifacts and noise</li> <li>Multiple fonts and sizes</li> <li>Mixed language documents</li> <li>Various document types</li> </ul> <p><strong>Solution</strong>: Comprehensive augmentation pipeline with realistic variations</p> <h2 id="looking-forward">Looking Forward</h2> <p>With the Meta Llama Impact Grant, we’re expanding:</p> <ol> <li><strong>More Languages</strong>: Adding support for African, Southeast Asian languages</li> <li><strong>Better Models</strong>: Training state-of-the-art VLMs on our dataset</li> <li><strong>Real-World Deployment</strong>: Partnering with organizations serving low-resource communities</li> <li><strong>Open Source</strong>: Making everything freely available to researchers worldwide</li> </ol> <h2 id="the-bigger-picture">The Bigger Picture</h2> <p>AI should work for everyone, regardless of what language they speak. The Nayana project is a step toward that vision.</p> <p>When AI systems can understand and generate content in any language, we unlock opportunities for billions of people. That’s the future we’re building.</p> <hr/> <p><strong>Collaborators</strong>: Adithya S Kolavi and Vyoman Jain at CognitiveLab</p> <p><strong>Resources</strong>:</p> <ul> <li>Dataset: Available on Hugging Face</li> <li>Code: Open-source on GitHub</li> <li>Papers: Available on arXiv</li> </ul> <p>The journey continues, and we’re just getting started. 🌍</p>]]></content><author><name></name></author><category term="research"/><category term="AI"/><category term="multilingual-AI"/><category term="NLP"/><category term="computer-vision"/><category term="meta-grant"/><summary type="html"><![CDATA[How our work on the Nayana dataset earned a $100,000 Meta Llama Impact Grant and why multilingual AI matters]]></summary></entry><entry><title type="html">a post with image galleries</title><link href="https://samarth777.github.io/blog/2024/photo-gallery/" rel="alternate" type="text/html" title="a post with image galleries"/><published>2024-12-04T01:59:00+00:00</published><updated>2024-12-04T01:59:00+00:00</updated><id>https://samarth777.github.io/blog/2024/photo-gallery</id><content type="html" xml:base="https://samarth777.github.io/blog/2024/photo-gallery/"><![CDATA[<p>The images in this post are all zoomable, arranged into different mini-galleries using different libraries.</p> <h2 id="lightbox2"><a href="https://lokeshdhakar.com/projects/lightbox2/">Lightbox2</a></h2> <p><a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p> <hr/> <h2 id="photoswipe"><a href="https://photoswipe.com/">PhotoSwipe</a></h2> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--getting-started"> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-pswp-width="1669" data-pswp-height="2500" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg" alt=""/> </a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-2500.jpg" data-pswp-width="1875" data-pswp-height="2500" data-cropped="true" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-200.jpg" alt=""/> </a> <a href="https://unsplash.com" data-pswp-src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1666" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg" alt=""/> </a> <div> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1667" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg" alt=""/> </a> </div> </div> <hr/> <h2 id="spotlight-js"><a href="https://nextapps-de.github.io/spotlight/">Spotlight JS</a></h2> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/> </a> </div> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg"/> </a> </div> <hr/> <h2 id="venobox"><a href="https://veno.es/venobox/">Venobox</a></h2> <p><a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included image galleries could look like]]></summary></entry><entry><title type="html">Manimator: From Research Papers to Visual Magic</title><link href="https://samarth777.github.io/blog/2024/manimator-launch/" rel="alternate" type="text/html" title="Manimator: From Research Papers to Visual Magic"/><published>2024-11-15T13:00:00+00:00</published><updated>2024-11-15T13:00:00+00:00</updated><id>https://samarth777.github.io/blog/2024/manimator-launch</id><content type="html" xml:base="https://samarth777.github.io/blog/2024/manimator-launch/"><![CDATA[<h2 id="the-problem">The Problem</h2> <p>Research papers are dense. Mathematical concepts can be abstract. And for many learners, the gap between reading a paper and truly <em>understanding</em> it feels insurmountable.</p> <p>What if we could automatically transform research papers into visual, animated explanations? That’s exactly what we set out to build with <strong>Manimator</strong>.</p> <h2 id="what-is-manimator">What is Manimator?</h2> <p>Manimator is an AI-powered tool that takes research papers from arXiv or mathematical concepts as input and generates animated visual explanations using the Manim engine (the same tool used by 3Blue1Brown for his famous math videos).</p> <h3 id="key-features">Key Features</h3> <ul> <li><strong>Automatic Animation Generation</strong>: Just provide an arXiv paper ID or describe a concept</li> <li><strong>Manim-Powered</strong>: Leverages the professional animation library used by top math educators</li> <li><strong>Scalable Pipeline</strong>: Built with FastAPI and Gradio for easy access</li> <li><strong>Docker Support</strong>: Deploy anywhere with containerization</li> </ul> <h2 id="the-launch">The Launch</h2> <p>We deployed Manimator on Hugging Face Spaces, and the response was overwhelming:</p> <ul> <li><strong>15,000+ uses within the first week</strong></li> <li><strong>Featured as Hugging Face’s Space of the Week</strong></li> <li>Used by researchers, students, and educators worldwide</li> </ul> <h2 id="technical-deep-dive">Technical Deep Dive</h2> <h3 id="the-pipeline">The Pipeline</h3> <ol> <li><strong>Input Processing</strong>: Parse arXiv papers or user-provided mathematical concepts</li> <li><strong>LLM Analysis</strong>: Use large language models to understand the core concepts</li> <li><strong>Scene Generation</strong>: Convert understanding into Manim animation code</li> <li><strong>Rendering</strong>: Execute Manim to create the final video</li> </ol> <h3 id="challenges-we-solved">Challenges We Solved</h3> <p><strong>Challenge 1: Understanding LaTeX Math</strong></p> <ul> <li>Research papers are full of complex mathematical notation</li> <li>Solution: Fine-tuned our prompts to handle LaTeX and mathematical reasoning</li> </ul> <p><strong>Challenge 2: Generating Valid Manim Code</strong></p> <ul> <li>Manim has a specific API and syntax</li> <li>Solution: Built a structured generation pipeline with validation steps</li> </ul> <p><strong>Challenge 3: Scalability</strong></p> <ul> <li>Rendering animations is compute-intensive</li> <li>Solution: Optimized with Docker and efficient resource management</li> </ul> <h2 id="real-world-impact">Real-World Impact</h2> <h3 id="education">Education</h3> <p>Students use Manimator to:</p> <ul> <li>Understand complex papers in their courses</li> <li>Create presentation materials</li> <li>Build intuition for abstract concepts</li> </ul> <h3 id="research-communication">Research Communication</h3> <p>Researchers leverage it to:</p> <ul> <li>Visualize their own work for presentations</li> <li>Make their papers more accessible</li> <li>Create supplementary materials for publications</li> </ul> <h2 id="what-we-learned">What We Learned</h2> <ol> <li><strong>Demand for Accessibility</strong>: There’s huge demand for tools that make technical content more accessible</li> <li><strong>Power of Visualization</strong>: Visual explanations significantly improve comprehension</li> <li><strong>Community Matters</strong>: Open-source deployment led to valuable feedback and use cases we never imagined</li> </ol> <h2 id="publication">Publication</h2> <p>Our work on Manimator was accepted at <strong>ICML 2025 NewInML Workshop</strong>, validating the research contribution behind the engineering effort.</p> <h2 id="try-it-yourself">Try It Yourself</h2> <p>Manimator is open-source and available on Hugging Face. Whether you’re a student trying to understand a difficult paper or a researcher wanting to visualize your work, give it a try!</p> <h2 id="whats-next">What’s Next?</h2> <p>We’re working on:</p> <ul> <li>Support for more paper formats (not just arXiv)</li> <li>Interactive animations where users can adjust parameters</li> <li>Integration with note-taking and learning management systems</li> <li>Better handling of figures and diagrams from original papers</li> </ul> <hr/> <p><strong>Collaborators</strong>: Vyoman Jain, Shiva Golugula, and Motamarri Sai Sathvik</p> <p>The future of learning is visual, interactive, and AI-powered. Manimator is just the beginning.</p>]]></content><author><name></name></author><category term="projects"/><category term="AI"/><category term="machine-learning"/><category term="visualization"/><category term="manim"/><summary type="html"><![CDATA[How we built an AI tool that transforms research papers into animated explanations and reached 15,000+ users in one week]]></summary></entry><entry><title type="html">HyperCluster: Democratizing LLMs Through P2P Computing</title><link href="https://samarth777.github.io/blog/2024/distributed-llms/" rel="alternate" type="text/html" title="HyperCluster: Democratizing LLMs Through P2P Computing"/><published>2024-10-10T20:00:00+00:00</published><updated>2024-10-10T20:00:00+00:00</updated><id>https://samarth777.github.io/blog/2024/distributed-llms</id><content type="html" xml:base="https://samarth777.github.io/blog/2024/distributed-llms/"><![CDATA[<h2 id="the-llm-accessibility-problem">The LLM Accessibility Problem</h2> <p>Large Language Models are powerful, but accessing them is difficult:</p> <p><strong>Cloud APIs</strong>:</p> <ul> <li>Expensive for extensive use</li> <li>Privacy concerns (data leaves your device)</li> <li>Requires internet connectivity</li> <li>Subject to rate limits and downtime</li> </ul> <p><strong>Local Deployment</strong>:</p> <ul> <li>Requires expensive GPUs (RTX 4090, A100)</li> <li>70B models need 140GB+ VRAM</li> <li>Most consumer hardware insufficient</li> <li>Single-device bottleneck</li> </ul> <p><strong>The Question</strong>: What if we could pool consumer hardware together to run LLMs collaboratively?</p> <p>Enter <strong>HyperCluster</strong>.</p> <h2 id="vision-peer-to-peer-llm-inference">Vision: Peer-to-Peer LLM Inference</h2> <h3 id="the-core-idea">The Core Idea</h3> <p>Instead of one powerful machine running the entire model:</p> <ol> <li><strong>Split the model</strong> across multiple devices</li> <li><strong>Distribute computation</strong> among peers</li> <li><strong>Coordinate inference</strong> through P2P network</li> <li><strong>Share resources</strong> dynamically</li> </ol> <p>Like BitTorrent for LLM inference.</p> <h3 id="why-this-matters">Why This Matters</h3> <p><strong>Democratization</strong>:</p> <ul> <li>Anyone with modest hardware can participate</li> <li>No need for expensive GPUs</li> <li>Collective computing power</li> </ul> <p><strong>Privacy</strong>:</p> <ul> <li>Data stays within your network</li> <li>No cloud dependency</li> <li>Full control over model and data</li> </ul> <p><strong>Resilience</strong>:</p> <ul> <li>No single point of failure</li> <li>Self-healing network</li> <li>Automatic peer discovery</li> </ul> <p><strong>Cost</strong>:</p> <ul> <li>Use existing hardware</li> <li>No cloud bills</li> <li>Community-driven</li> </ul> <h2 id="technical-architecture">Technical Architecture</h2> <h3 id="network-layer-iroh">Network Layer: Iroh</h3> <p>We use <strong>Iroh</strong> for P2P networking:</p> <p><strong>Key Features</strong>:</p> <ul> <li>NAT traversal (works behind routers)</li> <li>Automatic peer discovery</li> <li>Encrypted connections</li> <li>QUIC-based transport (fast and reliable)</li> <li>DHT for peer routing</li> </ul> <p><strong>Why Iroh?</strong>:</p> <ul> <li>Rust-native (performance + safety)</li> <li>Modern P2P protocol</li> <li>Active development</li> <li>Built for data-intensive applications</li> </ul> <h3 id="model-sharding">Model Sharding</h3> <p><strong>Challenge</strong>: Split a 70B parameter model across devices</p> <p><strong>Approach</strong>: Layer-wise sharding</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Device 1: Embedding + Layers 1-10
Device 2: Layers 11-20
Device 3: Layers 21-30
Device 4: Layers 31-40
Device 5: Layers 41-50
Device 6: Layers 51-60
Device 7: Layers 61-70
Device 8: Output layer
</code></pre></div></div> <p>Each device stores and computes a portion of the model.</p> <h3 id="quantization-bitnet">Quantization: BitNet</h3> <p>To reduce memory requirements:</p> <p><strong>Traditional Quantization</strong>:</p> <ul> <li>16-bit → 8-bit → 4-bit</li> <li>Still requires significant memory</li> </ul> <p><strong>BitNet</strong>:</p> <ul> <li><strong>1-bit weights</strong> (-1 or +1)</li> <li>Extreme compression</li> <li>Specialized inference kernels</li> <li>Minimal accuracy loss for certain models</li> </ul> <p><strong>Benefits</strong>:</p> <ul> <li>70B model → ~10GB (instead of 140GB)</li> <li>Fits on consumer devices</li> <li>Faster inference</li> <li>Lower bandwidth</li> </ul> <h3 id="inference-pipeline">Inference Pipeline</h3> <p><strong>Step 1: Request Arrives</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>User Query → Entry Node
</code></pre></div></div> <p><strong>Step 2: Token Processing</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>For each token:
  Embedding (Device 1) →
  Layer 1 (Device 1) →
  Layer 2 (Device 1) →
  ...
  Transfer to Device 2 →
  Layer 11 (Device 2) →
  ...
  Output (Device 8) →
  Token generated
</code></pre></div></div> <p><strong>Step 3: Streaming Response</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Tokens stream back to user as generated
</code></pre></div></div> <h3 id="dynamic-load-balancing">Dynamic Load Balancing</h3> <p><strong>Challenges</strong>:</p> <ul> <li>Devices have different capabilities</li> <li>Network conditions vary</li> <li>Some peers join/leave</li> </ul> <p><strong>Solution</strong>: Smart routing</p> <p><strong>Device Profiling</strong>:</p> <ul> <li>Measure compute speed</li> <li>Track network bandwidth</li> <li>Monitor availability</li> </ul> <p><strong>Adaptive Sharding</strong>:</p> <ul> <li>Assign more layers to faster devices</li> <li>Route around slow/offline peers</li> <li>Balance network traffic</li> </ul> <p><strong>Predictive Scheduling</strong>:</p> <ul> <li>Anticipate bottlenecks</li> <li>Pre-load data on fast paths</li> <li>Optimize critical paths</li> </ul> <h2 id="fault-tolerance">Fault Tolerance</h2> <h3 id="self-healing-network">Self-Healing Network</h3> <p><strong>Peer Failure Detection</strong>:</p> <ul> <li>Heartbeat monitoring</li> <li>Timeout thresholds</li> <li>Graceful degradation</li> </ul> <p><strong>Automatic Recovery</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Peer 3 goes offline
  ↓
Detect failure (heartbeat timeout)
  ↓
Find replacement peer
  ↓
Transfer model shard
  ↓
Resume inference
</code></pre></div></div> <p><strong>Redundancy</strong>:</p> <ul> <li>Critical layers replicated</li> <li>Multiple peers for popular layers</li> <li>Fast failover</li> </ul> <h3 id="state-management">State Management</h3> <p><strong>Checkpointing</strong>:</p> <ul> <li>Save intermediate activations</li> <li>Resume from checkpoints on failure</li> <li>Minimize wasted computation</li> </ul> <p><strong>Coordination</strong>:</p> <ul> <li>Distributed consensus (Raft)</li> <li>Leader election</li> <li>Consistent state across peers</li> </ul> <h2 id="llamacpp-integration">llama.cpp Integration</h2> <h3 id="why-llamacpp">Why llama.cpp?</h3> <ul> <li><strong>Efficient C++ implementation</strong> of LLM inference</li> <li><strong>Quantization support</strong> (GGUF format)</li> <li><strong>CPU optimization</strong> (important for consumer hardware)</li> <li><strong>Active community</strong></li> <li><strong>Broad model support</strong></li> </ul> <h3 id="our-modifications">Our Modifications</h3> <p><strong>1. Network-Aware Execution</strong></p> <ul> <li>Split execution across network calls</li> <li>Serialize/deserialize tensors efficiently</li> <li>Minimize data transfer</li> </ul> <p><strong>2. Streaming Support</strong></p> <ul> <li>Token-by-token generation across network</li> <li>Low latency response start</li> <li>Pipeline parallelism</li> </ul> <p><strong>3. Distributed KV Cache</strong></p> <ul> <li>Share key-value cache across peers</li> <li>Reduce redundant computation</li> <li>Memory efficiency</li> </ul> <h2 id="current-status">Current Status</h2> <p>HyperCluster is <strong>actively under development</strong>. Here’s where we are:</p> <h3 id="-completed">✅ Completed</h3> <ul> <li>P2P networking with Iroh</li> <li>Basic peer discovery</li> <li>Model sharding prototype</li> <li>Simple inference pipeline</li> <li>Fault detection</li> </ul> <h3 id="-in-progress">🚧 In Progress</h3> <ul> <li>BitNet quantization integration</li> <li>Dynamic load balancing</li> <li>Advanced fault tolerance</li> <li>Performance optimization</li> <li>Security hardening</li> </ul> <h3 id="-planned">📋 Planned</h3> <ul> <li>Multiple model architectures</li> <li>Web interface for node management</li> <li>Incentive mechanisms (tokenomics?)</li> <li>Mobile device support</li> <li>Edge device integration</li> </ul> <h2 id="challenges">Challenges</h2> <h3 id="1-network-bandwidth">1. Network Bandwidth</h3> <p><strong>Problem</strong>: Transferring activations between layers is expensive</p> <p><strong>Mitigation</strong>:</p> <ul> <li>Aggressive compression</li> <li>Predictive prefetching</li> <li>Local caching</li> <li>Network-aware sharding</li> </ul> <h3 id="2-heterogeneous-hardware">2. Heterogeneous Hardware</h3> <p><strong>Problem</strong>: Devices vary wildly in capability</p> <p><strong>Mitigation</strong>:</p> <ul> <li>Adaptive sharding (give more layers to fast devices)</li> <li>Task-appropriate assignment</li> <li>Continuous profiling</li> </ul> <h3 id="3-security">3. Security</h3> <p><strong>Problem</strong>: Malicious peers could:</p> <ul> <li>Send incorrect computations</li> <li>Steal data</li> <li>Disrupt network</li> </ul> <p><strong>Mitigation</strong>:</p> <ul> <li>Computation verification</li> <li>Encrypted connections</li> <li>Reputation systems</li> <li>Trusted peer networks</li> </ul> <h3 id="4-latency">4. Latency</h3> <p><strong>Problem</strong>: Network hops add latency</p> <p><strong>Mitigation</strong>:</p> <ul> <li>Pipeline parallelism</li> <li>Speculative execution</li> <li>Optimize routing</li> <li>Local first strategy</li> </ul> <h2 id="benchmarks-preliminary">Benchmarks (Preliminary)</h2> <p><strong>Setup</strong>: 70B model, 8 consumer devices (RTX 3060, 12GB each)</p> <table> <thead> <tr> <th>Metric</th> <th>HyperCluster</th> <th>Local (A100)</th> <th>Cloud API</th> </tr> </thead> <tbody> <tr> <td>Tokens/sec</td> <td>8-12</td> <td>30-40</td> <td>20-30</td> </tr> <tr> <td>Cost</td> <td>$0</td> <td>$20k hardware</td> <td>$0.03/1k tokens</td> </tr> <tr> <td>Privacy</td> <td>Full</td> <td>Full</td> <td>None</td> </tr> <tr> <td>Latency</td> <td>150-200ms</td> <td>50ms</td> <td>100-150ms</td> </tr> </tbody> </table> <p><strong>Takeaway</strong>: Slower than dedicated hardware, but:</p> <ul> <li>Accessible to everyone</li> <li>Zero cost for inference</li> <li>Private and censorship-resistant</li> </ul> <h2 id="use-cases">Use Cases</h2> <h3 id="1-community-ai">1. Community AI</h3> <p><strong>Local Communities</strong>:</p> <ul> <li>Share computing resources</li> <li>Run models collectively</li> <li>No corporate intermediaries</li> </ul> <p><strong>Examples</strong>:</p> <ul> <li>Neighborhood AI collective</li> <li>University research groups</li> <li>Hacker spaces</li> </ul> <h3 id="2-privacy-sensitive-applications">2. Privacy-Sensitive Applications</h3> <p><strong>Healthcare</strong>:</p> <ul> <li>Medical diagnosis without cloud</li> <li>Patient data stays local</li> <li>HIPAA compliance</li> </ul> <p><strong>Legal</strong>:</p> <ul> <li>Document analysis</li> <li>Client confidentiality</li> <li>Regulatory compliance</li> </ul> <h3 id="3-censorship-resistance">3. Censorship Resistance</h3> <p><strong>Uncensorable AI</strong>:</p> <ul> <li>No central control</li> <li>Distributed and resilient</li> <li>Community governance</li> </ul> <h3 id="4-edge-computing">4. Edge Computing</h3> <p><strong>IoT Networks</strong>:</p> <ul> <li>Smart city infrastructure</li> <li>Industrial automation</li> <li>Autonomous vehicle fleets</li> </ul> <h2 id="the-broader-vision">The Broader Vision</h2> <p>HyperCluster is part of a larger movement: <strong>Decentralized AI</strong>.</p> <h3 id="why-decentralization-matters">Why Decentralization Matters</h3> <p><strong>Concentration of Power</strong>:</p> <ul> <li>Few companies control powerful models</li> <li>Gatekeeping access</li> <li>Deciding what’s acceptable use</li> </ul> <p><strong>Decentralization Offers</strong>:</p> <ul> <li>Democratic access</li> <li>Community ownership</li> <li>Innovation without permission</li> <li>Resilience to censorship</li> </ul> <h3 id="inspiration">Inspiration</h3> <p><strong>BitTorrent</strong>: Showed P2P file sharing works at scale</p> <p><strong>IPFS</strong>: Demonstrated distributed content addressing</p> <p><strong>Folding@home</strong>: Proved distributed computing for science</p> <p><strong>HyperCluster</strong>: Bringing these ideas to AI inference</p> <h2 id="open-source-philosophy">Open Source Philosophy</h2> <p>HyperCluster will be <strong>fully open source</strong>:</p> <ul> <li>Code: MIT licensed</li> <li>Documentation: Comprehensive guides</li> <li>Community: Discord, GitHub discussions</li> <li>Contributions: Welcoming all skill levels</li> </ul> <h3 id="join-us">Join Us</h3> <p>Interested in contributing?</p> <p><strong>Needed Skills</strong>:</p> <ul> <li>Rust programming</li> <li>P2P networking</li> <li>ML inference optimization</li> <li>Distributed systems</li> </ul> <p><strong>Non-technical Contributions</strong>:</p> <ul> <li>Documentation</li> <li>Testing</li> <li>Community building</li> <li>Use case exploration</li> </ul> <h2 id="philosophical-reflections">Philosophical Reflections</h2> <h3 id="ai-should-be-public-infrastructure">AI Should Be Public Infrastructure</h3> <p>Like roads, bridges, and the internet, AI is becoming essential infrastructure. Should it be controlled by a few corporations?</p> <p>HyperCluster argues: <strong>No</strong>.</p> <h3 id="computing-as-a-community-resource">Computing as a Community Resource</h3> <p>We already share:</p> <ul> <li>Internet bandwidth (mesh networks)</li> <li>Storage (IPFS, torrents)</li> <li>Computing (BOINC, Folding@home)</li> </ul> <p>Why not share AI inference?</p> <h3 id="the-edge-computing-future">The Edge Computing Future</h3> <p>Cloud computing is powerful, but:</p> <ul> <li>Centralizes data</li> <li>Creates dependencies</li> <li>Increases latency</li> <li>Raises privacy concerns</li> </ul> <p>Edge computing (including P2P) offers an alternative:</p> <ul> <li>Data stays local</li> <li>Resilient to outages</li> <li>Lower latency</li> <li>Privacy by design</li> </ul> <h2 id="whats-next">What’s Next</h2> <h3 id="short-term-6-months">Short Term (6 months)</h3> <ol> <li><strong>Alpha Release</strong>: Working prototype for Llama 2</li> <li><strong>Benchmarking</strong>: Comprehensive performance analysis</li> <li><strong>Documentation</strong>: Setup and usage guides</li> <li><strong>Community</strong>: Build initial user base</li> </ol> <h3 id="medium-term-1-year">Medium Term (1 year)</h3> <ol> <li><strong>Multiple Models</strong>: Support for Mistral, GPT variants</li> <li><strong>Mobile Support</strong>: Run inference on phones</li> <li><strong>Incentive Layer</strong>: Token rewards for contributing compute</li> <li><strong>Production Ready</strong>: Stable, secure, performant</li> </ol> <h3 id="long-term-2-years">Long Term (2+ years)</h3> <ol> <li><strong>Mainstream Adoption</strong>: Thousands of nodes</li> <li><strong>Commercial Use</strong>: Businesses using HyperCluster</li> <li><strong>Research Platform</strong>: Academics studying distributed AI</li> <li><strong>Ecosystem</strong>: Third-party tools and services</li> </ol> <h2 id="conclusion">Conclusion</h2> <p>Large language models don’t have to be centralized. With clever engineering and community cooperation, we can:</p> <ul> <li>Run powerful models on modest hardware</li> <li>Maintain privacy and control</li> <li>Democratize access to AI</li> <li>Build resilient, censorship-resistant systems</li> </ul> <p>HyperCluster is our contribution to this vision. It’s ambitious, technically challenging, and absolutely necessary.</p> <p>The future of AI should be open, distributed, and owned by everyone.</p> <p>Join us in building it.</p> <hr/> <p><strong>Status</strong>: Active Development</p> <p><strong>Tech Stack</strong>: Rust, Iroh, llama.cpp, BitNet</p> <p><strong>Contribute</strong>: GitHub (coming soon)</p> <p><strong>Contact</strong>: Let’s collaborate on distributed AI!</p> <p>⚡🌐 <em>Computing power to the people</em> 🌐⚡</p>]]></content><author><name></name></author><category term="projects"/><category term="distributed-systems"/><category term="LLMs"/><category term="p2p"/><category term="edge-computing"/><summary type="html"><![CDATA[Building a peer-to-peer distributed framework for running large language models on consumer hardware]]></summary></entry><entry><title type="html">KissanDial: Bringing AI to Rural India via Voice Calls</title><link href="https://samarth777.github.io/blog/2024/ai-for-farmers/" rel="alternate" type="text/html" title="KissanDial: Bringing AI to Rural India via Voice Calls"/><published>2024-08-05T15:00:00+00:00</published><updated>2024-08-05T15:00:00+00:00</updated><id>https://samarth777.github.io/blog/2024/ai-for-farmers</id><content type="html" xml:base="https://samarth777.github.io/blog/2024/ai-for-farmers/"><![CDATA[<h2 id="the-digital-divide">The Digital Divide</h2> <p>India has over 140 million farmers. Many live in rural areas with:</p> <ul> <li>Limited smartphone access</li> <li>Poor internet connectivity</li> <li>Low digital literacy</li> <li>Language barriers (prefer local languages over English)</li> </ul> <p>Yet, farmers need timely information about:</p> <ul> <li>Government subsidies and schemes</li> <li>Weather forecasts for crop planning</li> <li>Market prices for their produce</li> <li>Best agricultural practices</li> <li>Pest and disease management</li> </ul> <p><strong>The question</strong>: How do we bring AI to those who need it most but have the least access to technology?</p> <h2 id="the-solution-kissandial">The Solution: KissanDial</h2> <p><strong>KissanDial</strong> is a voice call-based AI agent specifically designed for farmers. No smartphone needed. No internet required. Just a simple phone call.</p> <h3 id="how-it-works">How It Works</h3> <ol> <li><strong>Farmer calls a number</strong> (landline or basic mobile phone)</li> <li><strong>AI greets in local language</strong> (Hindi, Kannada, Tamil, etc.)</li> <li><strong>Farmer asks questions</strong> naturally in their language</li> <li><strong>AI provides information</strong> through voice responses</li> <li><strong>Follow-up questions</strong> supported for deeper understanding</li> </ol> <h3 id="what-it-provides">What It Provides</h3> <p><strong>Government Subsidies</strong></p> <ul> <li>Information about available schemes</li> <li>Eligibility criteria</li> <li>Application processes</li> <li>Required documents</li> </ul> <p><strong>Weather Updates</strong></p> <ul> <li>Localized weather forecasts</li> <li>Rainfall predictions</li> <li>Advisory for sowing/harvesting</li> <li>Extreme weather alerts</li> </ul> <p><strong>Market Information</strong></p> <ul> <li>Current crop prices in nearby mandis</li> <li>Best time to sell produce</li> <li>Demand trends</li> </ul> <p><strong>Agricultural Knowledge</strong></p> <ul> <li>Crop selection advice</li> <li>Pest management</li> <li>Fertilizer recommendations</li> <li>Irrigation best practices</li> </ul> <h2 id="technical-architecture">Technical Architecture</h2> <h3 id="voice-infrastructure">Voice Infrastructure</h3> <p><strong>Twilio Integration</strong></p> <ul> <li>Handles incoming calls</li> <li>Manages call routing</li> <li>Provides call recording for quality</li> </ul> <p><strong>Speech-to-Text (STT)</strong></p> <ul> <li>Converts farmer’s voice to text</li> <li>Supports multiple Indian languages</li> <li>Handles rural accents and dialects</li> </ul> <p><strong>Text-to-Speech (TTS)</strong></p> <ul> <li>Natural-sounding voice responses</li> <li>Regional language support</li> <li>Adjustable speaking rate for clarity</li> </ul> <h3 id="ai-agent-system">AI Agent System</h3> <p><strong>LlamaIndex Framework</strong></p> <ul> <li>Orchestrates the AI agent workflow</li> <li>Manages conversation context</li> <li>Routes queries to appropriate knowledge sources</li> </ul> <p><strong>Knowledge Base</strong></p> <ul> <li>Government scheme documents</li> <li>Agricultural extension material</li> <li>Weather API integration</li> <li>Market price databases</li> </ul> <p><strong>Vector Search (Milvus)</strong></p> <ul> <li>Fast retrieval of relevant information</li> <li>Semantic search for natural queries</li> <li>Handles variations in how questions are asked</li> </ul> <h3 id="backend">Backend</h3> <p><strong>FastAPI Server</strong></p> <ul> <li>RESTful API for call handling</li> <li>WebSocket for real-time updates</li> <li>Admin dashboard for monitoring</li> </ul> <h2 id="unique-challenges">Unique Challenges</h2> <h3 id="challenge-1-language-diversity">Challenge 1: Language Diversity</h3> <p>India has 22 official languages and hundreds of dialects.</p> <p><strong>Solution</strong>:</p> <ul> <li>Multilingual STT/TTS models</li> <li>Language detection</li> <li>Fallback to English when needed</li> <li>Continuous addition of new languages</li> </ul> <h3 id="challenge-2-low-literacy">Challenge 2: Low Literacy</h3> <p>Many farmers can’t read, so purely text-based solutions don’t work.</p> <p><strong>Solution</strong>:</p> <ul> <li>Voice-only interface</li> <li>Simple, conversational flow</li> <li>No need to navigate menus</li> <li>Natural language understanding</li> </ul> <h3 id="challenge-3-network-quality">Challenge 3: Network Quality</h3> <p>Rural areas often have poor phone connectivity with dropped calls.</p> <p><strong>Solution</strong>:</p> <ul> <li>Save conversation state</li> <li>Resume from where we left off</li> <li>Compress audio for low bandwidth</li> <li>Retry mechanisms</li> </ul> <h3 id="challenge-4-context-and-personalization">Challenge 4: Context and Personalization</h3> <p>Different farmers have different needs based on:</p> <ul> <li>Location (soil, climate)</li> <li>Crops grown</li> <li>Farm size</li> <li>Resources available</li> </ul> <p><strong>Solution</strong>:</p> <ul> <li>Location-aware responses</li> <li>Remember farmer preferences</li> <li>Contextual follow-ups</li> <li>Personalized recommendations</li> </ul> <h2 id="real-world-impact">Real-World Impact</h2> <h3 id="farmer-testimonials">Farmer Testimonials</h3> <blockquote> <p>“Earlier, I had to travel 20 km to the government office to ask about subsidies. Now I can call and get information immediately.” - Farmer in Karnataka</p> </blockquote> <blockquote> <p>“The weather information helped me plan my sowing better. I avoided losses this season.” - Farmer in Maharashtra</p> </blockquote> <h3 id="metrics">Metrics</h3> <ul> <li><strong>Calls handled</strong>: Thousands per month</li> <li><strong>Languages supported</strong>: 5+ (and growing)</li> <li><strong>Average call duration</strong>: 3-4 minutes</li> <li><strong>Information accuracy</strong>: High, validated by agricultural experts</li> </ul> <h3 id="recognition">Recognition</h3> <p><strong>Featured by LlamaIndex</strong> as an innovative application of AI agents in agriculture, showcasing practical use of AI for social good.</p> <h2 id="technical-learnings">Technical Learnings</h2> <h3 id="1-design-for-constraints">1. Design for Constraints</h3> <p>Rural India taught me to build for:</p> <ul> <li>Low bandwidth</li> <li>Limited device capabilities</li> <li>Intermittent connectivity</li> <li>Minimal technical literacy</li> </ul> <p>These constraints drive innovation.</p> <h3 id="2-context-is-everything">2. Context is Everything</h3> <p>Generic AI assistants don’t work. We needed:</p> <ul> <li>Domain-specific knowledge (agriculture)</li> <li>Regional context (local crops, climate)</li> <li>Cultural sensitivity (communication style)</li> </ul> <h3 id="3-voice-is-powerful">3. Voice is Powerful</h3> <p>For many rural users, voice is not just convenient - it’s the <strong>only</strong> accessible interface.</p> <h3 id="4-trust-takes-time">4. Trust Takes Time</h3> <p>Farmers need to trust the system. We build trust through:</p> <ul> <li>Accurate information</li> <li>Consistent availability</li> <li>Respectful communication</li> <li>Local language support</li> </ul> <h2 id="whats-next">What’s Next</h2> <h3 id="expanding-capabilities">Expanding Capabilities</h3> <p><strong>Pest Identification</strong></p> <ul> <li>Farmer describes pest/disease</li> <li>AI identifies the issue</li> <li>Provides treatment recommendations</li> </ul> <p><strong>Soil Testing Integration</strong></p> <ul> <li>Connect with soil testing labs</li> <li>Provide results via phone</li> <li>Customized fertilizer advice</li> </ul> <p><strong>Credit and Insurance</strong></p> <ul> <li>Information about agricultural loans</li> <li>Crop insurance schemes</li> <li>Eligibility and application support</li> </ul> <p><strong>Community Features</strong></p> <ul> <li>Connect farmers with agricultural experts</li> <li>Share best practices</li> <li>Group calls for training</li> </ul> <h3 id="scaling">Scaling</h3> <p><strong>More Languages</strong>: Adding Bengali, Punjabi, Gujarati <strong>More Regions</strong>: Expanding beyond current pilot areas <strong>Partnerships</strong>: Collaborating with NGOs and government agencies <strong>Offline Capability</strong>: SMS-based fallback for critical alerts</p> <h2 id="broader-implications">Broader Implications</h2> <p>KissanDial demonstrates how AI can be adapted for:</p> <p><strong>Low-Tech Environments</strong></p> <ul> <li>Not everyone has smartphones or internet</li> <li>AI should be accessible via existing infrastructure</li> </ul> <p><strong>Social Impact</strong></p> <ul> <li>Technology should serve those who need it most</li> <li>Commercial viability isn’t the only measure of success</li> </ul> <p><strong>Inclusive Design</strong></p> <ul> <li>Consider literacy, language, and cultural context</li> <li>One size doesn’t fit all</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>AI’s potential isn’t just in sophisticated chatbots for tech-savvy users. The real impact comes from making technology accessible to everyone, especially those traditionally excluded from the digital revolution.</p> <p>KissanDial is our small contribution to bridging that gap. One phone call at a time, we’re bringing AI to rural India.</p> <hr/> <p><strong>Tech Stack</strong>: LlamaIndex, Twilio, FastAPI, Milvus, Python</p> <p><strong>Status</strong>: Active project, continuously improving</p> <p><strong>Partners</strong>: Working with agricultural extension services and farmer cooperatives</p> <p>If you’re interested in AI for social good or building inclusive technology, let’s connect! 🌾📞</p>]]></content><author><name></name></author><category term="projects"/><category term="AI"/><category term="social-impact"/><category term="agriculture"/><category term="voice-AI"/><summary type="html"><![CDATA[Building a voice-based AI agent to help farmers access critical agricultural information]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://samarth777.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://samarth777.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://samarth777.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024 We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry></feed>